# Build a Chatbot that Speaks Real-Time with Any Voice

In this tutorial, you'll learn how to build a ChatGPT-like chatbot that not only responds to user queries in real-time but also speaks the responses using a customizable voice. You can even use your voice! The chatbot will highlight the words it is currently speaking, providing an immersive and interactive experience. We'll use Next.js with server-sent events (SSE) for low-latency communication, and Mantine for UI components. By using server-sent events, we can keep deployment quick and easy thanks to the serverless architecture.

<img
  src="/screenshot.png"
  alt="Chatbot that speaks real-time screenshot"
  style={{
    display: 'block',
    margin: '0 auto',
    maxWidth: '600px',
    width: '100%',
    height: 'auto',
  }}
/>

Want to see it in action? Jump to the [demo](#demo), or follow this tutorial to build this yourself.

## Prerequisites

Before we start, ensure you have the following:

- Node.js installed
- API keys for OpenAI and ElevenLabs
- A voice id from ElevenLabs (if you want to try a particular voice)

## Overview of the Data Flow

There's a lot of streaming involved in this project to keep everything low-latency. It may seem a bit complicated, but in practice, there's only two streams that matter: text and audio. Following where each is going individually will help you understand the logic better.

Here's a quick overview of all that's going on:

1. When a new user message comes in, we send it to OpenAI to get a streamed text response via a server-sent event.
2. We pipe that text response into ElevenLabs' text-to-speech WebSocket API and also send it back to the front-end via a server-sent event.
3. Whenever the ElevenLabs WebSocket sends audio chunks, we also send them to the front-end via server-side events.
4. In the front-end, whenever we receive text chunks from OpenAI, we display them in the last bot message.
5. In the front-end, whenever we receive audio chunks from ElevenLabs, we pipe them to an audio playing service.
6. The audio playing service adds chunks to its buffer and captures text-audio alignment information provided in the audio chunks.
7. While the audio playing service plays from its buffer, it emits events indicating what text is currently being read.
8. The React component highlights the word in the currently streamed bot message.

Could this be simplified by batching everything? Absolutely! But then our end result wouldn't be so awesome. ðŸ˜Ž

## Project Setup

First, let's bootstrap our Next.js project with Mantine:

```bash
npx create-next-app@latest --example with-mantine chatbot
cd chatbot
npm install
touch .env.local
```

Mantine is a UI-component library that will allow us to leverage some UI boilerplate. You can use any library you want. Now, let's add the logic.

Make sure to add these environment variables to your .env.local:

```text
OPENAI_API_KEY=
ELEVENLABS_API_KEY=
#ELEVENLABS_VOICE_ID=
```

If you want to use your own voice, simply remove the comment (hashtag) and add your voice id.

## Step 1: Create the Chatbot Component

The `Chatbot` component handles user input and displays messages. This is the core of our frontend logic.

Create `components/Chatbot.tsx` and add the following:

```typescript
// components/Chatbot.tsx
import { useState } from 'react';
import {
  Box,
  Button,
  Card,
  ScrollArea,
  Stack,
  Text,
  TextInput,
  useMantineTheme,
} from '@mantine/core';
import useChatbot, { Message as MessageType } from '../hooks/useChatbot'; // Import custom hook
import Message from './Message';

const Chatbot = () => {
  const [input, setInput] = useState('');
  const theme = useMantineTheme();

  const { messages, startListening, setMessages, startIndex, endIndex, audioPlayerRef } =
    useChatbot(); // Use the custom hook

  const handleSend = () => {
    audioPlayerRef.current?.stopAndFlush(); // Stop audio playback if necessary
    const newMessages: MessageType[] = [...messages, { content: input, role: 'user' }];
    setMessages(newMessages); // Update message state
    startListening(newMessages); // Start the EventSource listener
    setInput(''); // Clear input field
  };

  return (
    <Card
      style={{
        minHeight: 400,
        overflow: 'hidden',
        display: 'flex',
        width: '100%',
        flexDirection: 'column',
        borderRadius: theme.radius.md,
        backgroundColor: theme.colors.gray[0],
      }}
    >
      <ScrollArea style={{ flex: 1, padding: theme.spacing.md, overflow: 'auto' }}>
        <Text ta="center" fw={500} size="lg">
          Your-Voice-Enabled Chatbot
        </Text>
        <Stack gap="md" mt="xl">
          {messages.map((msg, index) => (
            <Message
              key={index}
              content={msg.content}
              role={msg.role as 'user' | 'assistant'}
              isLast={index === messages.length - 1}
              startIndex={startIndex}
              endIndex={endIndex}
            />
          ))}
        </Stack>
      </ScrollArea>
      <Box mt="md" style={{ display: 'flex', gap: theme.spacing.sm, padding: theme.spacing.sm }}>
        <TextInput
          value={input}
          onChange={(e) => setInput(e.currentTarget.value)}
          placeholder="Type your message"
          style={{ flexGrow: 1 }}
          onKeyUp={(e) => {
            if (e.key === 'Enter') {
              handleSend();
            }
          }}
        />
        <Button onClick={handleSend} variant="default" color="gray">Send</Button>
      </Box>
    </Card>
  );
};

export default Chatbot;
```

### Explanation

- **State Management**: Here, we use `useState` to manage user input and messages. This helps us keep track of what the user has typed and what messages have been exchanged.
- **Event Source**: The `startListening` function initializes an EventSource to listen for server-sent events from the backend. This allows us to receive real-time updates from the server, keeping our text stream flowing smoothly.
- **React Components**: The text input and messages components are what the users see and interact with. We keep the logic for messages in their own file for clarity.

## Step 2: Create the Message Component

The `Message` component is responsible for displaying individual messages. It handles the formatting and highlights the spoken words.

Create `components/Message.tsx` and add the following:

```typescript
// components/Message.tsx
import { IconRobotFace } from '@tabler/icons-react';
import { Group, Mark, rem, Text } from '@mantine/core';

interface MessageProps {
  content: string;
  role: 'user' | 'assistant';
  isLast: boolean;
  startIndex: number | null;
  endIndex: number | null;
}

const Message = ({ content, role, isLast, startIndex, endIndex }: MessageProps) => {

  // Conditionally handle highlighted text only for the last message
  const renderContent = () => {
    if (isLast && startIndex !== null && endIndex !== null) {
      return (
        <Text span >
          <Text span>{content.slice(0, startIndex)}</Text>
          <Mark color="grape">{content.slice(startIndex, endIndex)}</Mark>
          <Text span>{content.slice(endIndex)}</Text>
        </Text>
      );
    }

    // Render normal content for non-last messages or when indices are not defined
    return (
      <Text >{content}</Text>
    );
  };

  return (
    <Group
      justify={role === 'user' ? 'flex-end' : 'flex-start'}
      align="start"
      style={{
        whiteSpace: 'pre-wrap',
      }}

    >
      {role === 'assistant' && (
        <IconRobotFace
          style={{ width: rem(24), height: rem(24) }}
        />
      )}
      {renderContent()}
    </Group>
  );
};

export default Message;
```

### Explanation

- **Conditional Rendering**: The `renderContent` function highlights text only for the last message. This ensures that only the currently spoken words are highlighted, giving users visual feedback on what the chatbot is saying.
- **Styling**: We use Mantine's `Group` component to align content horizontally and an icon at the top left, mimicing ChatGPT. This makes our text stream visually appealing and make the user feel at home.

## Step 3: Implement the Custom Hook

The `useChatbot` custom hook manages the chatbot's state and handles communication with the server. This encapsulates the logic for maintaining the chat state and interacting with the backend.

Create `hooks/useChatbot.ts` and add the following:

```typescript
// hooks/useChatbot.ts
import { useEffect, useState, useRef } from 'react';
import { Base64AudioPlayer } from '../classes/Base64AudioPlayer';

export interface Message {
  content: string;
  role: 'user' | 'assistant';
}

const useChatbot = () => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [eventSource, setEventSource] = useState<EventSource | null>(null);
  const [startIndex, setStartIndex] = useState<number | null>(null);
  const [endIndex, setEndIndex] = useState<number | null>(null);
  const audioPlayerRef = useRef<Base64AudioPlayer | null>(null);

  // Initialize the audio player and set startIndex, endIndex handlers
  useEffect(() => {
    audioPlayerRef.current = new Base64AudioPlayer((startIdx, endIdx) => {
      setStartIndex(startIdx);
      setEndIndex(endIdx);
    });

    return () => {
      audioPlayerRef.current?.stopAndFlush();
    };
  }, []);

  const startListening = (newMessages: Message[]) => {
    const urlEncodedMessages = encodeURIComponent(JSON.stringify(newMessages));

    // Close any previous EventSource connections
    if (eventSource) {
      eventSource.close();
    }

    const url = `/api/chat?messages=${urlEncodedMessages}`;
    const newEventSource = new EventSource(url);
    setEventSource(newEventSource);

    audioPlayerRef.current?.resumeIfSuspended(); // browser requirement

    // Handle incoming messages from the server
    newEventSource.onmessage = (event) => {
      try {
        const data = JSON.parse(event.data);

        if (data.text) {
          setMessages((prevMessages) => {
            const lastMessage = prevMessages[prevMessages.length - 1];
            if (lastMessage && lastMessage.role === 'assistant') {
              // Append to the last assistant message
              return [
                ...prevMessages.slice(0, -1),
                {
                  content: lastMessage.content + data.text,
                  role: 'assistant',
                },
              ];
            } else {
              // Create a new assistant message
              return [...prevMessages, { content: data.text, role: 'assistant' }];
            }
          });
        } else if (data.audio) {
          audioPlayerRef.current?.addChunk(data);
        }
      } catch (error) {
        console.error('Failed to parse SSE message:', error);
      }
    };

    newEventSource.onerror = (error) => {
      console.error('EventSource error:', error);
      newEventSource.close();
    };

    return () => {
      newEventSource.close();
    };
  };

  return {
    messages,
    startListening,
    setMessages,
    startIndex,
    endIndex,
    audioPlayerRef,
  };
};

export default useChatbot;
```

### Explanation

- **State Management**: This hook manages messages, EventSource, and audio playback, keeping our component clean and focused on rendering. It allows us to efficiently handle both text and audio streams.
- **Event Handling**: The hook processes incoming messages and audio chunks from the server. This allows us to handle real-time updates efficiently, ensuring that both our text and audio streams are in sync.

## Step 4: Create the API Route

The API route handles incoming chat messages and communicates with OpenAI and ElevenLabs. This is where the backend logic resides.

Create `pages/api/chat.ts` and add the following:

```typescript
// pages/api/chat.ts
import { NextApiRequest, NextApiResponse } from 'next';
import OpenAI from 'openai';
import WebSocket from 'ws';
import { Alignment } from '../../classes/ReadingIndex';

const openai = new OpenAI();
const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;
const VOICE_ID = process.env.ELEVENLABS_VOICE_ID ?? '21m00Tcm4TlvDq8ikWAM';

// Helper function to split text into chunks without breaking sentences
async function* textChunker(chunks: AsyncIterable<string>): AsyncGenerator<string> {
  const splitters = /\n/; // Regular expression for valid split characters
  let buffer = '';

  for await (const text of chunks) {
    buffer += text;

    let splitIndex = -1;
    // Find the last valid splitting point in the buffer
    for (let i = buffer.length - 1; i >= 0; i--) {
      if (splitters.test(buffer[i])) {
        splitIndex = i;
        break;
      }
    }

    // If a splitting point is found, yield the chunk and update the buffer
    if (splitIndex !== -1) {
      yield buffer.slice(0, splitIndex + 1); // Yield up to the splitter
      buffer = buffer.slice(splitIndex + 1); // Keep the remaining part in the buffer
    }
  }

  // Yield any remaining text
  if (buffer) {
    yield buffer;
  }
}

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  // Set SSE headers
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');

  try {
    // Extract messages from URL parameters
    const messages = JSON.parse(req.query.messages as string) as OpenAI.ChatCompletionMessage[];

    if (messages.length === 0) {
      res.status(400).json({ error: 'Message parameter is required' });
      return;
    }

    // 1. Establish WebSocket connection to ElevenLabs
    const elevenLabsWs = new WebSocket(
      `wss://api.elevenlabs.io/v1/text-to-speech/${VOICE_ID}/stream-input?model_id=eleven_turbo_v2_5`
    );

    let accumulatedChunk: { audio: string; alignment: Alignment } | null = null; // To accumulate audio data if no alignment is set

    elevenLabsWs.on('open', async () => {
      console.log('Connected to ElevenLabs WebSocket');

      // Send initial voice settings (optional)
      elevenLabsWs.send(
        JSON.stringify({
          text: ' ',
          voice_settings: { stability: 0.8, similarity_boost: 0.8 },
          xi_api_key: ELEVENLABS_API_KEY,
        })
      );

      // 2. Start OpenAI streaming after the WebSocket connection is established
      const openAiStream = await openai.chat.completions.create({
        messages: [{ role: 'system', content: 'You are a helpful assistant.' }, ...messages],
        model: 'gpt-4o-mini',
        stream: true,
      });

      // Create a generator for the OpenAI text stream
      const chunkedStream = textChunker(
        (async function* () {
          for await (const chunk of openAiStream) {
            const delta = chunk.choices[0]?.delta?.content;
            if (delta) {
              yield delta;
            }
          }
        })()
      );

      for await (const textChunk of chunkedStream) {
        // 3. Send the properly chunked text to ElevenLabs for speech synthesis
        elevenLabsWs.send(JSON.stringify({ text: textChunk }));

        // Also stream the text to the client in real-time
        res.write(`data: ${JSON.stringify({ text: textChunk })}\n\n`);
      }

      console.log('OpenAI stream ended.');
      elevenLabsWs.send(JSON.stringify({ text: '' })); // Signal the end of text to ElevenLabs
    });

    // 4. Listen for audio responses from ElevenLabs and stream them to the client
    elevenLabsWs.on('message', (data: string) => {
      try {
        const parsedData = JSON.parse(data);

        if (parsedData.audio) {
          const chunk = { audio: parsedData.audio, alignment: parsedData.alignment };

          if (parsedData.alignment) {
            // Send the accumulated chunk if a new alignment is received
            if (accumulatedChunk) {
              res.write(`data: ${JSON.stringify(accumulatedChunk)}\n\n`);
              accumulatedChunk = chunk; // Clear accumulated chunk
            } else {
              accumulatedChunk = chunk; // Update the current chunk with alignment
            }
          } else {
            if (accumulatedChunk) {
              accumulatedChunk.audio = Buffer.concat([
                Buffer.from(accumulatedChunk.audio, 'base64'),
                Buffer.from(chunk.audio, 'base64'),
              ]).toString('base64');
            } else {
              res.write(`data: ${JSON.stringify(chunk)}\n\n`);
            }
          }
        }
      } catch (error) {
        console.error('Error processing ElevenLabs WebSocket message:', error);
      }
    });

    elevenLabsWs.on('close', () => {
      console.log('ElevenLabs WebSocket closed.');
      // Send the last accumulated chunk if available
      if (accumulatedChunk) {
        res.write(`data: ${JSON.stringify(accumulatedChunk)}\n\n`);
      }
      res.end(); // Ensure the HTTP response ends if the WebSocket closes
    });

    elevenLabsWs.on('error', (err) => {
      console.error('ElevenLabs WebSocket error:', err);
      res.status(500).json({ error: 'ElevenLabs WebSocket connection error' });
    });
  } catch (error) {
    console.error('Error in handler:', error);
    res.status(500).json({ error: 'An error occurred' });
  }
}
```

### Explanation

- **SSE Headers**: We set headers for server-sent events to ensure the client can receive real-time updates. This is crucial for keeping our text stream flowing.
- **WebSocket Connection**: We connect to ElevenLabs for real-time speech synthesis, allowing us to convert text to speech in real-time. This is where our audio stream starts.
- **OpenAI Streaming**: We stream responses from OpenAI and send them to ElevenLabs and the client. This ensures low-latency communication, keeping both our text and audio streams synchronized.

### Using Your Own Voice

If you'd like to use your own voice, simply create one in ElevenLabs, go to `Voices`, select your voice, then click the `ID` button to copy its id. Paste it in the code above.

## Step 5: Create the Audio Player Class

The `Base64AudioPlayer` class handles audio playback from Base64 encoded strings. This is essential for playing the audio received from ElevenLabs.

Create `pages/classes/Base64AudioPlayer.ts` and add the following:

```typescript
// pages/classes/Base64AudioPlayer.ts
import { Alignment, ReadingIndex } from './ReadingIndex';

export type AudioChunk = {
  audio: string;
  alignment?: Alignment;
};

export class Base64AudioPlayer {
  private audioQueue: AudioChunk[] = [];
  private isPlaying: boolean = false;
  private audioContext: AudioContext;
  private sourceNode: AudioBufferSourceNode | null = null;
  private readingIndex: ReadingIndex;

  constructor(callback: (startIndex: number, endIndex: number) => void) {
    this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    this.readingIndex = new ReadingIndex(callback);
  }

  public resumeIfSuspended(): void {
    if (this.audioContext.state === 'suspended') {
      this.audioContext.resume();
    }
  }

  public async addChunk(chunk: AudioChunk): Promise<void> {
    this.audioQueue.push(chunk);

    if (chunk.alignment) {
      this.readingIndex.addAlignment(chunk.alignment);
    }

    if (!this.isPlaying) {
      this.isPlaying = true;
      this.readingIndex.startChecking();
      await this.playQueue();
    }
  }

  private async playQueue(): Promise<void> {
    while (this.audioQueue.length > 0) {
      const chunk = this.audioQueue.shift();
      if (chunk) {
        await this.playChunk(chunk);
      }
    }
    this.isPlaying = false;
    this.readingIndex.stopChecking();
  }

  private async playChunk(chunk: AudioChunk): Promise<void> {
    return new Promise((resolve, reject) => {
      const audioData = this.base64ToArrayBuffer(chunk.audio);

      this.audioContext.decodeAudioData(
        audioData,
        (buffer) => {
          this.sourceNode = this.audioContext.createBufferSource();
          this.sourceNode.buffer = buffer;
          this.sourceNode.connect(this.audioContext.destination);
          this.sourceNode.start(0);
          this.sourceNode.onended = () => {
            resolve();
          };
        },
        (error) => {
          console.error('Error decoding audio data', error);
          reject(error);
        }
      );
    });
  }

  public stopAndFlush(): void {
    if (this.sourceNode) {
      this.sourceNode.stop();
      this.sourceNode.disconnect();
      this.sourceNode = null;
    }
    this.isPlaying = false;
    this.audioQueue = [];
    this.readingIndex.stopChecking();
  }

  private base64ToArrayBuffer(base64: string): ArrayBuffer {
    const binaryString = window.atob(base64);
    const len = binaryString.length;
    const bytes = new Uint8Array(len);
    for (let i = 0; i < len; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes.buffer;
  }
}
```

### Explanation

- **AudioContext**: We use the Web Audio API for audio playback, which provides a robust way to handle audio in the browser. This is where our audio stream gets played.
- **Queue Management**: This class manages a queue of audio chunks to ensure smooth playback. By adding chunks to the queue, we ensure that the audio plays seamlessly, maintaining a good user experience.

## Step 6: Create the Reading Index Class

The `ReadingIndex` class manages the alignment of text and audio for highlighting. This ensures that the correct words are highlighted as they are spoken.

Create `pages/classes/ReadingIndex.ts` and add the following:

```typescript
// pages/classes/ReadingIndex.ts
export type Alignment = {
  chars: string[]; // Array of characters in the audio chunk
  charStartTimesMs: number[]; // Start times of each character in the audio chunk
  charDurationsMs: number[]; // Durations of each character in the audio chunk
};

export class ReadingIndex {
  private runAlignment: Alignment | null = null;
  private startTime: number | null = null;
  private checkWordingInterval: NodeJS.Timeout | null = null;
  private callback: (startIndex: number, endIndex: number) => void;

  constructor(callback: (startIndex: number, endIndex: number) => void) {
    this.callback = callback;
  }

  public startChecking(): void {
    if (this.startTime === null) {
      this.startTime = Date.now();
      this.checkWordingInterval = setInterval(() => {
        this.checkAlignment();
      }, 100);
    }
  }

  public stopChecking(): void {
    if (this.checkWordingInterval) {
      clearInterval(this.checkWordingInterval);
      this.checkWordingInterval = null;
    }
    this.runAlignment = null;
    this.startTime = null;
    this.callback(-1, -1);
  }

  public addAlignment(chunkAlignment: Alignment): void {
    if (!this.runAlignment) {
      this.runAlignment = chunkAlignment;
    } else {
      // Calculate the offset: last start time + last duration
      const lastStartTime =
        this.runAlignment.charStartTimesMs.length > 0
          ? this.runAlignment.charStartTimesMs[this.runAlignment.charStartTimesMs.length - 1]
          : 0;
      const lastDuration =
        this.runAlignment.charDurationsMs.length > 0
          ? this.runAlignment.charDurationsMs[this.runAlignment.charDurationsMs.length - 1]
          : 0;
      const offset = lastStartTime + lastDuration;

      // Adjust the new chunk's start times by adding the offset
      const adjustedStartTimes = chunkAlignment.charStartTimesMs.map(
        (startTime) => startTime + offset
      );

      // Concatenate the adjusted values, ensuring there's space where needed
      const atLeastOneSpaceOrNewLine =
        ['\n', ' '].includes(this.runAlignment.chars[this.runAlignment.chars.length - 1]) ||
        ['\n', ' '].includes(chunkAlignment.chars[0]);

      if (!atLeastOneSpaceOrNewLine) {
        this.runAlignment.chars.push(' ', ...chunkAlignment.chars);
        this.runAlignment.charStartTimesMs.push(adjustedStartTimes[0] - 1, ...adjustedStartTimes);
        this.runAlignment.charDurationsMs.push(1, ...chunkAlignment.charDurationsMs);
      } else {
        this.runAlignment.chars.push(...chunkAlignment.chars);
        this.runAlignment.charStartTimesMs.push(...adjustedStartTimes);
        this.runAlignment.charDurationsMs.push(...chunkAlignment.charDurationsMs);
      }
    }
  }

  private checkAlignment(): void {
    if (!this.runAlignment || !this.startTime) return;

    const currentTime = Date.now() - this.startTime;
    let charIndex = this.runAlignment.charStartTimesMs.findIndex(
      (startTime, idx) =>
        currentTime >= startTime && currentTime < startTime + this.runAlignment!.charDurationsMs[idx]
    );

    // if it's a space, we want to go back to the last word
    if (charIndex > 0 && this.runAlignment.chars[charIndex] === ' ') {
      charIndex--;
    }

    if (charIndex === -1) {
      // If we overshot and no character matches, ensure we don't go out of bounds.
      charIndex = this.runAlignment.charStartTimesMs.length - 1;
    }

    if (charIndex > 0) {
      // Get the first space or period before and after the current character
      let startIndex = charIndex - 1;
      let endIndex = charIndex - 1;
      while (startIndex >= 0 && this.runAlignment.chars[startIndex] !== ' ') {
        startIndex--;
      }
      while (
        endIndex < this.runAlignment.chars.length &&
        this.runAlignment.chars[endIndex] !== ' '
      ) {
        endIndex++;
      }

      this.callback(Math.max(0, startIndex), endIndex);
    }
  }
}
```

### Explanation

- **Alignment Management**: This class keeps track of the start and end times of each character in the audio, ensuring accurate highlighting. This is essential for syncing our text and audio streams.
- **Callback Handling**: The class calls a callback function to highlight the current word being spoken, providing real-time feedback to the user. This enhances the interactivity of our chatbot.

## Step 7: Integrate Components in the Home Page

Finally, integrate the `Chatbot` component in your home page. This brings everything together and makes the chatbot accessible to the user.

Create `pages/index.tsx` and add the following:

```typescript
// pages/index.tsx
import { Container, Grid } from '@mantine/core';
import Chatbot from '@/components/Chatbot';

const Home = () => {
  return (
    <>     
      <main>
        <Container>
          <Chatbot />
        </Container>
      </main>
    </>
  );
};

export default Home;
```

### Explanation

- **Layout**: We use Mantine's `Container` for consistent layout, ensuring a clean and responsive design. This makes our chatbot look great on any device.
- **Component Inclusion**: By including the `Chatbot` component, we make the chatbot available on the home page, allowing users to interact with it right away.

## Running the Project and Deploying it

To run the project locally, run:

```bash
npm run dev
```

Then, go to localhost:3000.

To deploy the project, I recommend you give Vercel access to your project, provide the three environment variables, and hit deploy. You might need to set the install command to `npm i`.

You can find detailed instructions for this [here](https://nextjs.org/learn-pages-router/basics/deploying-nextjs-app/deploy).

---

Congratulations. You've built a sophisticated chatbot that responds in real-time, speaks the responses, and highlights the words being spoken. This project showcases the power of Next.js, Mantine, OpenAI, and ElevenLabs, providing a rich, interactive user experience.

Deploy your project to Vercel or any other platform that supports serverless functions to make your chatbot accessible to the world.

Happy coding.

## The Full Code, and Contributing

If you'd like, feel free to fork the [repo for this project](https://github.com/valdrox/chat-streamer), and deploy it on Vercel! All you need is to give Vercel access to your project, provide the three environment variables, configure install as `npm i` and hit deploy. You can find instructions for this [here](https://nextjs.org/learn-pages-router/basics/deploying-nextjs-app/deploy).

I also welcome any contributions, particularly UI focused. ðŸ˜‰